{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Recurrent Neural Network on MNIST Dataset.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "NQfz_UlIwHm6",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "2f766478-9d3a-4869-f02f-b6c7fc82f41e",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525935108745,
          "user_tz": -330,
          "elapsed": 7250,
          "user": {
            "displayName": "Bobby Rathore",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118066803566018997213"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "from tensorflow.python.ops import rnn_cell\n",
        "from tensorflow.contrib import rnn\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot = True)\n",
        "\n",
        "hm_epochs = 20\n",
        "n_classes = 10\n",
        "batch_size = 128\n",
        "chunk_size = 28\n",
        "n_chunks = 28\n",
        "rnn_size = 128\n",
        "\n",
        "\n",
        "x = tf.placeholder('float', [None, n_chunks,chunk_size])\n",
        "y = tf.placeholder('float')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-1-719472c32055>:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hEB3lI3uwbWq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def recurrent_neural_network(x):\n",
        "    \n",
        "    layer = {'weights': tf.Variable(tf.truncated_normal([rnn_size, n_classes], stddev=0.1)), 'biases': tf.Variable(tf.constant(0.1, shape=[n_classes]))}\n",
        "\n",
        "    x = tf.transpose(x, [1,0,2])\n",
        "    x = tf.reshape(x, [-1, chunk_size])\n",
        "    x = tf.split(x, n_chunks, 0)\n",
        "\n",
        "    lstm_cell = rnn_cell.BasicLSTMCell(rnn_size, state_is_tuple=True)\n",
        "    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
        "\n",
        "    output = tf.matmul(outputs[-1],layer['weights']) + layer['biases']\n",
        "\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RWCywD-kxnY3",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def train_neural_network(x):\n",
        "    prediction = recurrent_neural_network(x)\n",
        "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction, labels=y))\n",
        "    \n",
        "    # minimize cost\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
        "    \n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        for epoch in range(hm_epochs):\n",
        "            epoch_loss = 0\n",
        "            for _ in range(int(mnist.train.num_examples/batch_size)):\n",
        "                epoch_x, epoch_y = mnist.train.next_batch(batch_size)\n",
        "                epoch_x = epoch_x.reshape((batch_size,n_chunks,chunk_size))\n",
        "\n",
        "                _, c = sess.run([optimizer, cost], feed_dict={x: epoch_x, y: epoch_y})\n",
        "                epoch_loss += c\n",
        "\n",
        "            print('Epoch', epoch+1, 'completed out of',hm_epochs,'loss:',epoch_loss)\n",
        "\n",
        "        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
        "\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
        "        print('Accuracy:',accuracy.eval({x:mnist.test.images.reshape((-1, n_chunks, chunk_size)), y:mnist.test.labels})*100)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vJAOe4fDx9oY",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "3a96b787-1d40-4870-8b27-4978e73aa67e",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525935273291,
          "user_tz": -330,
          "elapsed": 157816,
          "user": {
            "displayName": "Bobby Rathore",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118066803566018997213"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Run\n",
        "train_neural_network(x)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 completed out of 20 loss: 240.69318786263466\n",
            "Epoch 2 completed out of 20 loss: 70.46602479368448\n",
            "Epoch 3 completed out of 20 loss: 48.40774934180081\n",
            "Epoch 4 completed out of 20 loss: 36.97551094181836\n",
            "Epoch 5 completed out of 20 loss: 28.967577564995736\n",
            "Epoch 6 completed out of 20 loss: 24.979740807320923\n",
            "Epoch 7 completed out of 20 loss: 20.301043623127043\n",
            "Epoch 8 completed out of 20 loss: 17.917350572068244\n",
            "Epoch 9 completed out of 20 loss: 16.620792207075283\n",
            "Epoch 10 completed out of 20 loss: 14.164775706012733\n",
            "Epoch 11 completed out of 20 loss: 13.543566430686042\n",
            "Epoch 12 completed out of 20 loss: 11.330714250681922\n",
            "Epoch 13 completed out of 20 loss: 10.71738048212137\n",
            "Epoch 14 completed out of 20 loss: 10.11939733615145\n",
            "Epoch 15 completed out of 20 loss: 9.893829475506209\n",
            "Epoch 16 completed out of 20 loss: 8.304935497464612\n",
            "Epoch 17 completed out of 20 loss: 8.245884006784763\n",
            "Epoch 18 completed out of 20 loss: 7.086872466374189\n",
            "Epoch 19 completed out of 20 loss: 6.63914691991522\n",
            "Epoch 20 completed out of 20 loss: 5.7501436036254745\n",
            "Accuracy: 98.39000105857849\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fYoceyGXyEdZ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}